apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: spice-runner-keda
  namespace: default
  labels:
    app: spice-runner
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: spice-runner
  
  # Scale from 1 to 20 pods
  # PRODUCTION VALUES:
  minReplicaCount: 1            # Keep at least 1 pod running (GCP Ingress requires backends)
  maxReplicaCount: 20           # Maximum 20 pods for production workload
  # DEMO VALUES (for live demonstrations): minReplicaCount: 1, maxReplicaCount: 200
  
  # Polling and cooldown settings  
  # PRODUCTION VALUES:
  pollingInterval: 30           # Check metrics every 30 seconds (reduces API load)
  cooldownPeriod: 300           # Wait 5 minutes before scaling down (prevents flapping)
  # DEMO VALUES (for live demonstrations): pollingInterval: 5, cooldownPeriod: 30
  
  # Fallback configuration (if metrics are unavailable)
  fallback:
    failureThreshold: 3
    replicas: 1                 # Fallback to 1 replica if metrics fail
  
  triggers:
  # ============================================================================
  # Trigger 1: HTTP Request Rate
  # Each active game session generates ~0.5-1 req/s from heartbeats and events
  # ============================================================================
  - type: prometheus
    metricType: AverageValue    # Required for fallback configuration
    metadata:
      serverAddress: http://prometheus.observability.svc.cluster.local:9090
      metricName: http_requests_per_second
      # Query total HTTP request rate
      query: |
        sum(rate(nginx_http_requests_total{service="spice-runner-nginx"}[30s])) or vector(0)
      # PRODUCTION VALUES:
      threshold: '10'           # Scale 1 pod per 10 req/s (more efficient utilization)
      activationThreshold: '1'  # Wake from zero at 1 req/s (requires actual traffic)
      # DEMO VALUES (for live demonstrations): threshold: '1', activationThreshold: '0.2'
  
  # ============================================================================
  # Trigger 2: CPU Safety Net (prevent overload)
  # ============================================================================
  - type: cpu
    metricType: Utilization     # Use percentage utilization
    metadata:
      # PRODUCTION VALUE:
      value: '70'               # Scale up if CPU exceeds 70% (allows good utilization)
      # DEMO VALUE (for live demonstrations): value: '50'

---
# ============================================================================
# OBSERVABILITY STACK AUTOSCALING
# ============================================================================

# Grafana ScaledObject - Scale based on CPU and Memory
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: grafana-keda
  namespace: observability
  labels:
    app: grafana
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: grafana
  
  # Scale from 1 to 5 Grafana instances
  minReplicaCount: 1
  maxReplicaCount: 5
  
  # Polling and cooldown settings
  pollingInterval: 15           # Check metrics every 15 seconds
  cooldownPeriod: 300           # Wait 5 minutes before scaling down (Grafana needs stability)
  
  triggers:
  # CPU-based scaling
  - type: cpu
    metricType: Utilization
    metadata:
      value: '80'               # Scale up when average CPU > 80%
  
  # Memory-based scaling
  - type: memory
    metricType: Utilization
    metadata:
      value: '80'               # Scale up when average memory > 80%

---
# Prometheus ScaledObject - Vertical scaling awareness (can add more triggers later)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-keda
  namespace: observability
  labels:
    app: prometheus
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: prometheus
  
  # Keep at 1 replica (PVC constraint), but KEDA can monitor for alerts
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  # CPU monitoring (won't scale but good for visibility)
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # Very high threshold since we can't horizontally scale
  
  # Memory monitoring
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # Very high threshold since we can't horizontally scale

---
# Loki ScaledObject - Vertical scaling awareness
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: loki-keda
  namespace: observability
  labels:
    app: loki
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: loki
  
  # Keep at 1 replica (PVC constraint)
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only
  
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only

---
# Tempo ScaledObject - Vertical scaling awareness
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: tempo-keda
  namespace: observability
  labels:
    app: tempo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tempo
  
  # Keep at 1 replica (PVC constraint)
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only
  
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only

