apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: spice-runner-keda
  namespace: default
  labels:
    app: spice-runner
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: spice-runner
  
  # Scale from 1 to 150 pods (DEMO MODE - will trigger massive node scaling!)
  minReplicaCount: 1
  maxReplicaCount: 150
  
  # Polling and cooldown settings
  pollingInterval: 10           # Check metrics every 10 seconds (faster response)
  cooldownPeriod: 120           # Wait 2 minutes before scaling down (faster for demo)
  
  # Fallback configuration (if metrics are unavailable)
  fallback:
    failureThreshold: 3
    replicas: 2                 # Fallback to 2 replicas if metrics fail
  
  triggers:
  # ============================================================================
  # Trigger 1: HTTP Request Rate (Primary - Event-Driven Scaling)
  # ============================================================================
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.observability.svc.cluster.local:9090
      metricName: http_requests_per_second
      query: |
        sum(rate(nginx_http_requests_total{service="spice-runner-nginx"}[1m])) or vector(0)
      threshold: '10'           # Scale up when > 10 requests/second (VERY aggressive for demo)
      activationThreshold: '1'  # Wake from zero at >= 1 request/second
      ignoreNullValues: 'true'  # Don't scale down if query returns null
  
  # ============================================================================
  # Trigger 2: CPU Utilization (Resource-Based Scaling)
  # ============================================================================
  - type: cpu
    metricType: Utilization     # Use percentage utilization
    metadata:
      value: '30'               # Scale up when average CPU > 30% (VERY aggressive for demo)
  
  # ============================================================================
  # Trigger 3: Memory Utilization (Resource-Based Scaling)
  # ============================================================================
  - type: memory
    metricType: Utilization     # Use percentage utilization
    metadata:
      value: '80'               # Scale up when average memory > 80%

---
# ============================================================================
# OBSERVABILITY STACK AUTOSCALING
# ============================================================================

# Grafana ScaledObject - Scale based on CPU and Memory
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: grafana-keda
  namespace: observability
  labels:
    app: grafana
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: grafana
  
  # Scale from 1 to 5 Grafana instances
  minReplicaCount: 1
  maxReplicaCount: 5
  
  # Polling and cooldown settings
  pollingInterval: 15           # Check metrics every 15 seconds
  cooldownPeriod: 300           # Wait 5 minutes before scaling down (Grafana needs stability)
  
  # Fallback configuration
  fallback:
    failureThreshold: 3
    replicas: 1
  
  triggers:
  # CPU-based scaling
  - type: cpu
    metricType: Utilization
    metadata:
      value: '60'               # Scale up when average CPU > 60%
  
  # Memory-based scaling
  - type: memory
    metricType: Utilization
    metadata:
      value: '70'               # Scale up when average memory > 70%

---
# Prometheus ScaledObject - Vertical scaling awareness (can add more triggers later)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-keda
  namespace: observability
  labels:
    app: prometheus
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: prometheus
  
  # Keep at 1 replica (PVC constraint), but KEDA can monitor for alerts
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  # CPU monitoring (won't scale but good for visibility)
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # Very high threshold since we can't horizontally scale
  
  # Memory monitoring
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # Very high threshold since we can't horizontally scale

---
# Loki ScaledObject - Vertical scaling awareness
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: loki-keda
  namespace: observability
  labels:
    app: loki
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: loki
  
  # Keep at 1 replica (PVC constraint)
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only
  
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only

---
# Tempo ScaledObject - Vertical scaling awareness
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: tempo-keda
  namespace: observability
  labels:
    app: tempo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tempo
  
  # Keep at 1 replica (PVC constraint)
  minReplicaCount: 1
  maxReplicaCount: 1
  
  pollingInterval: 30
  cooldownPeriod: 300
  
  triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only
  
  - type: memory
    metricType: Utilization
    metadata:
      value: '200'              # High threshold - monitoring only

